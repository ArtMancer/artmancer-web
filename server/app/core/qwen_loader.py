from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional

import torch
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
from diffusers import QwenImageEditPlusPipeline # type: ignore

from .config import settings

logger = logging.getLogger(__name__)

_pipeline_insertion: Optional[DiffusionPipeline] = None
_pipeline_removal: Optional[DiffusionPipeline] = None
_pipeline_wb: Optional[DiffusionPipeline] = None  # white-balance


def _ensure_file(path: str | Path, env_name: str) -> Path:
    """ƒê·∫£m b·∫£o file t·ªìn t·∫°i, n·∫øu kh√¥ng th√¨ b√°o l·ªói r√µ r√†ng."""
    model_path = Path(path)
    if not model_path.exists():
        raise FileNotFoundError(
            f"{env_name} tr·ªè t·ªõi file kh√¥ng t·ªìn t·∫°i: {model_path}. "
            f"H√£y ki·ªÉm tra ƒë∆∞·ªùng d·∫´n trong .env ({env_name})."
        )
    return model_path


def get_qwen_pipeline(task_type: str) -> Optional[DiffusionPipeline]:
    """L·∫•y pipeline Qwen t·ª´ cache n·∫øu ƒë√£ load."""
    if task_type == "removal":
        return _pipeline_removal
    if task_type == "white-balance":
        return _pipeline_wb
    # M·∫∑c ƒë·ªãnh d√πng pipeline insertion cho c√°c task c√≤n l·∫°i (insertion)
    return _pipeline_insertion


def load_qwen_pipeline(task_type: str = "insertion") -> DiffusionPipeline:
    """
    Load QwenImageEditPlusPipeline theo ƒë√∫ng style code tham chi·∫øu:
    - D√πng from_pretrained(model_name, torch_dtype=bfloat16)
    - Load LoRA t·ª´ MODEL_FILE_INSERTION / MODEL_FILE_REMOVAL / MODEL_FILE_WHITE_BALANCE
    - Kh√¥ng fallback sang pipeline kh√°c; l·ªói s·∫Ω raise th·∫≥ng ƒë·ªÉ d·ªÖ debug.
    """
    global _pipeline_insertion, _pipeline_removal, _pipeline_wb
    
    # Check cache
    cached = get_qwen_pipeline(task_type)
    if cached is not None:
        return cached
    
    # Base model t·ª´ Hugging Face (c√≥ th·ªÉ sau n√†y cho v√†o config n·∫øu c·∫ßn)
    base_model_id = "Qwen/Qwen-Image-Edit-2509"

    # Ch·ªçn file LoRA theo task (m·ªói task m·ªôt checkpoint ri√™ng)
    if task_type == "removal":
        lora_path = _ensure_file(settings.model_file_removal, "MODEL_FILE_REMOVAL")
    elif task_type == "white-balance":
        lora_path = _ensure_file(
            settings.model_file_white_balance,
            "MODEL_FILE_WHITE_BALANCE",
        )
    else:  # insertion (m·∫∑c ƒë·ªãnh)
        lora_path = _ensure_file(settings.model_file_insertion, "MODEL_FILE_INSERTION")
    
    # L·∫•y device & dtype
    from .pipeline import get_device
    
    device = get_device()
    dtype = torch.bfloat16 if device.type == "cuda" else torch.float32

    logger.info(
        "üîÑ Loading QwenImageEditPlusPipeline base model '%s' on %s (dtype=%s)",
        base_model_id,
        device,
        dtype,
    )

    try:
        pipe = QwenImageEditPlusPipeline.from_pretrained(
            base_model_id,
            torch_dtype=dtype,
        )
    except Exception as exc:
        raise RuntimeError(
            "Kh√¥ng th·ªÉ load QwenImageEditPlusPipeline t·ª´ "
            f"'{base_model_id}': {exc}. "
            "H√£y ki·ªÉm tra l·∫°i diffusers version v√† quy·ªÅn truy c·∫≠p Hugging Face."
        ) from exc

    pipe = pipe.to(device)

    # Load LoRA weights
    logger.info("üì• Loading LoRA weights from %s cho task=%s", lora_path, task_type)
    try:
        pipe.load_lora_weights(str(lora_path), adapter_name="lora")
        pipe.set_adapters("lora")
        pipe.fuse_lora(adapter_names=["lora"])
    except Exception as exc:
        raise RuntimeError(
            f"Kh√¥ng th·ªÉ load/fuse LoRA t·ª´ {lora_path}: {exc}. "
            "ƒê·∫£m b·∫£o ƒë√¢y l√† checkpoint LoRA t∆∞∆°ng th√≠ch v·ªõi QwenImageEditPlus."
        ) from exc
    
    logger.info("‚úÖ QwenImageEditPlusPipeline + LoRA ƒë√£ s·∫µn s√†ng cho task=%s", task_type)
    
    # Cache theo task
    if task_type == "removal":
        _pipeline_removal = pipe
    elif task_type == "white-balance":
        _pipeline_wb = pipe
    else:
        _pipeline_insertion = pipe
    
    return pipe


def clear_qwen_cache() -> None:
    """Clear cached Qwen pipelines."""
    global _pipeline_insertion, _pipeline_removal, _pipeline_wb
    if _pipeline_insertion is not None:
        del _pipeline_insertion
        _pipeline_insertion = None
    if _pipeline_removal is not None:
        del _pipeline_removal
        _pipeline_removal = None
    if _pipeline_wb is not None:
        del _pipeline_wb
        _pipeline_wb = None
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    logger.info("üßπ Cleared Qwen pipeline caches")


def is_qwen_pipeline_loaded(task_type: str | None = None) -> bool:
    """
    Check if Qwen pipeline is loaded without forcing a load.
    
    Args:
        task_type: "insertion", "removal", "white-balance", or None to check any pipeline
    
    Returns:
        True if pipeline is loaded, False otherwise
    """
    if task_type == "removal":
        return _pipeline_removal is not None
    if task_type == "white-balance":
        return _pipeline_wb is not None
    if task_type == "insertion":
        return _pipeline_insertion is not None
    # task_type is None: check b·∫•t k·ª≥ pipeline n√†o
    return (
        (_pipeline_insertion is not None)
        or (_pipeline_removal is not None)
        or (_pipeline_wb is not None)
    )

