"""
Metrics utilities for image evaluation.

C√°c metric v√† √Ω nghƒ©a (ƒë·∫ßu v√†o gi·∫£ ƒë·ªãnh l√† ·∫£nh ƒë√£ chu·∫©n h√≥a v·ªÅ [0, 1], d·∫°ng tensor (B, C, H, W)):

- PSNR (Peak Signal-to-Noise Ratio) [dB] ‚Äì Cao h∆°n l√† t·ªët:
  + ~20‚Äì30 dB: ch·∫•p nh·∫≠n ƒë∆∞·ª£c
  + >30 dB: t·ªët
  + >40 dB: r·∫•t t·ªët/xu·∫•t s·∫Øc
  L∆∞u √Ω: tƒÉng ~1 dB l√† c·∫£i thi·ªán nh·ªè; ~3 dB l√† ƒë√°ng k·ªÉ.

- SSIM (Structural Similarity) ‚àà [0, 1] ‚Äì Cao h∆°n l√† t·ªët:
  + >0.90: t·ªët
  + >0.95: r·∫•t t·ªët
  + 1.0: tr√πng kh·ªõp ho√†n h·∫£o

- LPIPS (Learned Perceptual Image Patch Similarity) ‚àà [0, 1] ‚Äì Th·∫•p h∆°n l√† t·ªët:
  + <0.20: kh√°
  + <0.10: t·ªët
  + <0.05: r·∫•t t·ªët
  L∆∞u √Ω: V·ªõi normalize=True, metric s·∫Ω t·ª± chu·∫©n h√≥a t·ª´ [0,1] sang [-1,1] n·ªôi b·ªô.

- ŒîE00 (Delta E 2000) ‚â• 0 ‚Äì Th·∫•p h∆°n l√† t·ªët (ƒëo sai kh√°c m√†u trong kh√¥ng gian c·∫£m nh·∫≠n):
  + <1.0: h·∫ßu nh∆∞ kh√¥ng th·ªÉ nh·∫≠n ra
  + 1.0‚Äì2.0: kh√≥ nh·∫≠n ra (quan s√°t k·ªπ)
  + 2.0‚Äì5.0: nh·∫≠n ra nh·∫π
  + 5.0‚Äì10.0: kh√°c bi·ªát r√µ
  + >10: kh√°c bi·ªát l·ªõn

G·ª£i √Ω s·ª≠ d·ª•ng:
- Duy tr√¨ chu·∫©n h√≥a ·∫£nh v·ªÅ [0, 1] tr∆∞·ªõc khi t√≠nh PSNR/SSIM/LPIPS.
- Batch size v√† device c·ªßa tensor n√™n kh·ªõp v·ªõi metric instance (ƒë√£ chuy·ªÉn .to(device)).
"""

from __future__ import annotations

import logging

import numpy as np
import torch
from PIL import Image
from skimage.color import deltaE_ciede2000, rgb2lab
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity

logger = logging.getLogger(__name__)

# Get device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize metric instances (assume images in [0, 1])
psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)
# LPIPS with AlexNet backbone as required
lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type="alex", normalize=True).to(device)
ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)

# Initialize CLIP model for CLIP-S (lazy loading)
_clip_model = None
_clip_preprocess = None

def _get_clip_model():
    """Lazy load CLIP model."""
    global _clip_model, _clip_preprocess
    if _clip_model is None:
        try:
            import clip
            _clip_model, _clip_preprocess = clip.load("ViT-B/32", device=device)
            _clip_model.eval()
            logger.info("‚úÖ CLIP model loaded for CLIP-S metric")
        except ImportError:
            logger.warning("‚ö†Ô∏è CLIP not available. Install with: pip install git+https://github.com/openai/CLIP.git")
            return None, None
    return _clip_model, _clip_preprocess

logger.info(f"üìä Metrics initialized on device: {device}")


def pil_to_tensor(img: Image.Image) -> torch.Tensor:
    """
    Convert PIL Image to torch.Tensor in format (B, C, H, W) with values in [0, 1].
    
    Args:
        img: PIL Image (RGB)
    
    Returns:
        Tensor of shape (1, 3, H, W) with values in [0, 1]
    """
    # Convert to RGB if needed
    if img.mode != "RGB":
        img = img.convert("RGB")
    
    # Convert to numpy array and normalize to [0, 1]
    img_array = np.array(img, dtype=np.float32) / 255.0
    
    # Convert to tensor and add batch dimension: (H, W, C) -> (1, C, H, W)
    tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
    
    return tensor.to(device)


def compute_psnr(img1: torch.Tensor, img2: torch.Tensor, metric_instance: PeakSignalNoiseRatio) -> torch.Tensor:
    """
    T√≠nh PSNR (dB). Cao h∆°n l√† t·ªët.
    
    ƒê·∫ßu v√†o: img1, img2 d·∫°ng (B, C, H, W), gi√° tr·ªã ‚àà [0, 1].
    G·ª£i √Ω m·ª©c t·ªët: >30 dB (t·ªët), >40 dB (r·∫•t t·ªët).
    """
    img1 = img1.to(metric_instance.device)
    img2 = img2.to(metric_instance.device)
    return metric_instance(img1, img2)


def compute_lpips(img1: torch.Tensor, img2: torch.Tensor, metric_instance: LearnedPerceptualImagePatchSimilarity) -> torch.Tensor:
    """
    T√≠nh LPIPS. Th·∫•p h∆°n l√† t·ªët.
    
    ƒê·∫ßu v√†o: img1, img2 d·∫°ng (B, C, H, W), ‚àà [0, 1]. V·ªõi normalize=True s·∫Ω t·ª± ƒë·ªïi v·ªÅ [-1, 1].
    G·ª£i √Ω m·ª©c t·ªët: <0.10 (t·ªët), <0.05 (r·∫•t t·ªët).
    """
    img1 = img1.to(metric_instance.device)
    img2 = img2.to(metric_instance.device)
    return metric_instance(img1, img2)


def compute_ssim(img1: torch.Tensor, img2: torch.Tensor, metric_instance: StructuralSimilarityIndexMeasure) -> torch.Tensor:
    """
    T√≠nh SSIM ‚àà [0, 1]. Cao h∆°n l√† t·ªët.
    
    ƒê·∫ßu v√†o: img1, img2 d·∫°ng (B, C, H, W), ‚àà [0, 1].
    """
    img1 = img1.to(metric_instance.device)
    img2 = img2.to(metric_instance.device)
    return metric_instance(img1, img2)


def compute_de00(img1: torch.Tensor, img2: torch.Tensor) -> float:
    """
    T√≠nh Delta E 2000 (ƒë·ªô sai kh√°c m√†u). Th·∫•p h∆°n l√† t·ªët.
    
    - ƒê·∫ßu v√†o: img1, img2 d·∫°ng (B, C, H, W), ‚àà [0, 1].
    - Tr·∫£ v·ªÅ: trung b√¨nh ŒîE00 c·ªßa batch (float, CPU).
    - <1.0: kh√¥ng th·ªÉ nh·∫≠n ra
    - 1.0‚Äì2.0: kh√≥ nh·∫≠n ra
    - 2.0‚Äì5.0: nh·∫≠n ra nh·∫π
    - 5.0‚Äì10.0: kh√°c bi·ªát r√µ
    - >10: kh√°c bi·ªát l·ªõn
    
    L∆∞u √Ω: H√†m ch·∫°y tr√™n CPU v√† chuy·ªÉn sang numpy; c√≥ th·ªÉ l√† bottleneck cho batch l·ªõn.
    """
    # Chuy·ªÉn v·ªÅ (B, H, W, C) cho skimage, gi·ªØ gi√° tr·ªã float [0, 1]
    img1_np = img1.permute(0, 2, 3, 1).cpu().numpy().astype(np.float32)
    img2_np = img2.permute(0, 2, 3, 1).cpu().numpy().astype(np.float32)
    
    # Clip to [0, 1] to ensure valid range
    img1_np = np.clip(img1_np, 0, 1)
    img2_np = np.clip(img2_np, 0, 1)
    
    batch_size = img1_np.shape[0]
    de00_scores = []
    
    for i in range(batch_size):
        lab1 = rgb2lab(img1_np[i])
        lab2 = rgb2lab(img2_np[i])
        de00 = deltaE_ciede2000(lab1, lab2)
        de00_scores.append(np.mean(de00))
    
    return float(np.mean(de00_scores))


def compute_clip_score(img1: Image.Image, img2: Image.Image) -> float | None:
    """
    Compute CLIP Similarity Score (CLIP-S).
    
    Args:
        img1: First image (PIL Image)
        img2: Second image (PIL Image)
    
    Returns:
        CLIP-S score (higher is better, typically in [0, 1]) or None if CLIP unavailable
    """
    clip_model, clip_preprocess = _get_clip_model()
    if clip_model is None or clip_preprocess is None:
        return None
    
    try:
        # Preprocess images
        img1_tensor = clip_preprocess(img1).unsqueeze(0).to(device)
        img2_tensor = clip_preprocess(img2).unsqueeze(0).to(device)
        
        # Get image features
        with torch.no_grad():
            img1_features = clip_model.encode_image(img1_tensor)
            img2_features = clip_model.encode_image(img2_tensor)
            
            # Normalize features
            img1_features = img1_features / img1_features.norm(dim=-1, keepdim=True)
            img2_features = img2_features / img2_features.norm(dim=-1, keepdim=True)
            
            # Compute cosine similarity
            clip_score = (img1_features @ img2_features.T).item()
        
        return float(clip_score)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error computing CLIP-S: {e}")
        return None


def compute_all_metrics(img1: Image.Image, img2: Image.Image) -> dict[str, float]:
    """
    Compute all metrics for a pair of PIL Images.
    
    Args:
        img1: Original image (PIL Image)
        img2: Target/reference image (PIL Image)
    
    Returns:
        Dictionary with all metric scores (PSNR, SSIM, LPIPS, CLIP-S, ŒîE00)
    """
    # Ensure same size
    if img1.size != img2.size:
        logger.info(f"üîÑ Resizing images to match: {img1.size} -> {img2.size}")
        img2 = img2.resize(img1.size, Image.Resampling.LANCZOS)
    
    # Convert to tensors
    tensor1 = pil_to_tensor(img1)
    tensor2 = pil_to_tensor(img2)
    
    # Compute metrics
    with torch.no_grad():
        psnr_score = compute_psnr(tensor1, tensor2, psnr_metric).item()
        lpips_score = compute_lpips(tensor1, tensor2, lpips_metric).item()
        ssim_score = compute_ssim(tensor1, tensor2, ssim_metric).item()
        de00_score = compute_de00(tensor1, tensor2)
    
    # Compute CLIP-S (separate, as it uses different preprocessing)
    clip_score = compute_clip_score(img1, img2)
    
    result = {
        "psnr": float(psnr_score),
        "lpips": float(lpips_score),
        "ssim": float(ssim_score),
        "de00": float(de00_score),
    }
    
    if clip_score is not None:
        result["clip_score"] = clip_score
    
    return result

