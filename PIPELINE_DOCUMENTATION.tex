\subsection{Full Pipeline Demonstration}

The ArtMancer system implements a sophisticated reference-guided image editing pipeline that seamlessly integrates frontend user interactions with backend AI processing services. This section provides a comprehensive overview of the end-to-end workflow, demonstrating how user inputs are transformed into high-quality edited images through a multi-stage AI pipeline.

\subsubsection{User Interaction and Input Collection}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/frontend_interface.png}
    \caption{The ArtMancer frontend interface showing the main canvas with brush tool active. The interface displays the base image with a semi-transparent red overlay indicating the user-drawn placement mask (Mask A). The sidebar contains controls for model selection (FastSAM/BiRefNet), brush size adjustment, and generation parameters.}
    \label{fig:frontend_interface}
\end{figure}

The frontend interface, built with React and Next.js, provides an intuitive canvas-based editing environment where users can interact with images through multiple modalities. The system supports three distinct editing tasks: object insertion, object removal, and white balance correction, each with specific input requirements and processing workflows.

For object insertion and removal tasks, the main canvas enables users to draw placement masks (Mask A) directly on the base image using brush tools or bounding boxes, with real-time preview through semi-transparent red overlays that indicate selected regions. The mask defines the area where an object will be inserted (for insertion tasks) or removed (for removal tasks). For object insertion specifically, a dedicated modal interface, the Reference Image Editor, allows users to upload reference images and extract target objects using intelligent segmentation. Within this editor, users can employ brush strokes, bounding boxes, or automatic detection to isolate objects of interest, generating a reference mask (Mask R) that defines the object's shape and boundaries. The system offers two segmentation models for mask generation: FastSAM (segmentation) and BiRefNet, each optimized for different use cases. FastSAM excels at point-based and brush-guided segmentation, while BiRefNet provides superior accuracy for bounding box inputs on cropped regions.

For white balance correction, the system operates without masks, processing the entire input image to adjust color temperature and lighting conditions automatically.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/reference_image_editor.png}
    \caption{The Reference Image Editor modal interface. Left panel shows the reference image with brush strokes (red overlay) indicating the object to be extracted. Right panel displays the extracted object with transparent background. The toolbar includes model selection (Segmentation/BiRefNet), border adjustment, and auto-detect options.}
    \label{fig:reference_editor}
\end{figure}

When a user initiates the editing process, the frontend collects task-specific inputs based on the selected editing mode. For object insertion tasks, the required inputs include: the base image to be edited, the main mask (Mask A) defining the placement region on the base image, the reference image containing the object to be inserted, the reference mask (Mask R) isolating the target object from the reference image, a text prompt describing the desired transformation, and generation parameters. For object removal tasks, the inputs are: the base image to be edited, the main mask (Mask A) defining the region to be removed, a text prompt describing the removal, and generation parameters. For white balance correction, only the input image and generation parameters are required, as no masks are needed for this task. Common generation parameters across all tasks include inference steps, guidance scale, seed, and other configuration options.

\subsubsection{Frontend Processing and API Communication}

Prior to backend submission, the frontend performs several preprocessing steps to optimize the data pipeline. The drawing canvas state is captured using HTML5 Canvas API with an optimized \texttt{willReadFrequently} attribute for efficient pixel data extraction during mask generation. All image data, including the base image, masks, and reference image, are encoded in Base64 format for efficient transmission over HTTP. Multiple API calls are orchestrated sequentially, with mask generation occurring first, followed by object extraction, and finally image generation, ensuring proper data dependencies throughout the workflow. Server-Sent Events (SSE) are utilized for real-time progress updates during long-running generation tasks, providing users with visual feedback on processing stages.

The frontend communicates with the backend through a centralized API Gateway, which routes requests to appropriate microservices based on the operation type. This architecture ensures scalability and allows independent scaling of different service components.

\subsubsection{Backend Processing Pipeline}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/pipeline/full_pipeline_overview.png}
    \caption{Complete pipeline overview showing the end-to-end workflow from user input to final generated image. The diagram illustrates the sequential stages: mask generation, object extraction (for insertion tasks), image generation, and result delivery.}
    \label{fig:pipeline_overview}
\end{figure}

Upon receiving a generation request, the backend executes a multi-stage pipeline orchestrated by the Job Manager service:

\paragraph{Stage 1: Mask Generation and Refinement}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/fastsam_mask_generation.png}
        \caption{FastSAM mask generation from brush strokes}
        \label{fig:fastsam_mask}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/birefnet_mask_generation.png}
        \caption{BiRefNet two-stage refinement: FastSAM initial mask (left) and BiRefNet refined mask (right)}
        \label{fig:birefnet_mask}
    \end{subfigure}
    \caption{Comparison of mask generation approaches. (a) FastSAM directly processes brush strokes to generate masks. (b) BiRefNet employs a two-stage approach: FastSAM generates an initial mask from brush strokes, then BiRefNet refines the mask on the calculated bounding box region for superior boundary accuracy.}
    \label{fig:mask_generation}
\end{figure}

When the frontend submits mask generation requests for either the main mask or reference mask, the Segmentation Service processes the input according to the selected model and input type. For brush inputs with FastSAM, the system directly processes brush strokes as point-based prompts, generating precise masks that follow user-drawn regions. When BiRefNet is selected with brush inputs, a two-stage approach is employed: FastSAM first generates an initial mask from brush strokes, then the mask's bounding box is calculated with padding, and finally BiRefNet refines the mask on the cropped region, providing superior accuracy for object boundaries. For bounding box inputs, the system crops the specified region and applies the selected segmentation model, either FastSAM or BiRefNet, to generate masks. BiRefNet is particularly effective for this use case, as it operates on cropped regions and produces high-quality segmentation results.

The segmentation models are deployed on GPU-enabled containers (T4 GPUs) to ensure fast inference times. Mask post-processing includes morphological operations (opening, closing, dilation, erosion) for FastSAM outputs, while BiRefNet masks undergo minimal processing to preserve their inherent quality.

\paragraph{Stage 2: Reference Object Extraction (Object Insertion Only)}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/pipeline/object_extraction.png}
    \caption{Reference object extraction process. The Image Utils Service applies the reference mask (Mask R) to isolate the target object from the reference image. The object is then resized to a 1:1 ratio (512x512 or 1024x1024) and placed on a white background, creating the conditional image used in the generation process.}
    \label{fig:object_extraction}
\end{figure}

For object insertion tasks, the Image Utils Service extracts the target object from the reference image using the generated reference mask (Mask R). This process applies the binary mask to isolate the object, removes background pixels by setting them to black, and preserves the natural shape and proportions defined by Mask R. The extracted object is resized to a 1:1 ratio (512x512 or 1024x1024 depending on the object size) and placed on a white background, creating the conditional image used during generation. This extracted object is then used as a conditional input during the generation process. For object removal and white balance tasks, this stage is skipped as no reference image is required.

\paragraph{Stage 3: Task-Specific Image Generation}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pipeline/insertion_pipeline.png}
        \caption{Object insertion pipeline}
        \label{fig:insertion_pipeline}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pipeline/removal_pipeline.png}
        \caption{Object removal pipeline}
        \label{fig:removal_pipeline}
    \end{subfigure}
    \caption{Task-specific generation pipelines. (a) Object insertion: The reference mask is positioned within the main mask, and multiple conditional images guide the diffusion process. (b) Object removal: The masked region is inpainted using MAE representation to preserve context.}
    \label{fig:generation_pipelines}
\end{figure}

The core generation pipeline, executed on A100 GPUs, implements task-specific algorithms through distinct workflows. The system utilizes the Qwen-Image-Edit model with task-specific LoRA adapters (insertion, removal, or white-balance) to perform iterative denoising steps, guided by text prompts and conditional images.

For object insertion tasks, the pipeline implements a reference-guided insertion algorithm. First, mask positioning occurs where the reference mask (Mask R) is scaled and positioned within the main mask (Mask A) on the base image, creating a positioned mask that defines where the object should be inserted. Conditional image preparation involves creating multiple conditional images: the reference image (resized to match base dimensions), the main mask A indicating the placement region, and the masked background showing the base image with the placement region removed. The system utilizes ControlNet with multiple condition types to guide the diffusion process and ensure precise object placement and integration.

For object removal tasks, the pipeline focuses on inpainting the masked region. Conditional images include the original base image, the main mask A indicating the region to be removed, and a masked autoencoder (MAE) representation that helps preserve surrounding context. The model generates content to fill the masked region seamlessly, maintaining consistency with the surrounding image.

For white balance correction tasks, the pipeline processes the entire image without masks. Conditional images include the input image and Canny edge detection results, which help preserve image structure while adjusting color temperature and lighting conditions. The model applies global color corrections to achieve natural white balance.

Throughout all generation processes, intermediate results are streamed to the frontend via SSE, allowing users to observe the generation process in real-time.

\paragraph{Stage 4: Result Delivery and Debug Information}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/pipeline/result_delivery.png}
    \caption{Result delivery and debug information flow. The system streams intermediate results via SSE during generation, then returns the final image along with comprehensive debug information including all conditional images, positioned masks, and generation metadata for analysis and reproducibility.}
    \label{fig:result_delivery}
\end{figure}

Upon completion, the system returns the final generated image in Base64 encoding, along with generation metadata including the model used, parameters, and execution time. Additionally, comprehensive debug information is provided, containing all intermediate conditional images for analysis, as well as session information for debugging and reproducibility purposes.

\subsubsection{Multi-modal Editing Example}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/example_base_image.png}
        \caption{Base image: Wooden table}
        \label{fig:example_base}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/example_reference_image.png}
        \caption{Reference image: Remote control on orange mat}
        \label{fig:example_reference}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/example_result.png}
        \caption{Final result: Remote control inserted on table}
        \label{fig:example_result}
    \end{subfigure}
    \caption{Complete editing workflow example. (a) The base image shows a wooden table where the object will be placed. (b) The reference image contains the target object (remote control) on an orange mat. (c) The final generated image shows the remote control seamlessly inserted onto the wooden table, with natural lighting and shadows.}
    \label{fig:editing_example}
\end{figure}

To illustrate the pipeline's capabilities, we present examples for each of the three supported editing tasks.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/example_mask_a.png}
        \caption{Main mask A: Placement region on base image}
        \label{fig:example_mask_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/example_mask_r.png}
        \caption{Reference mask R: Object isolation from reference}
        \label{fig:example_mask_r}
    \end{subfigure}
    \caption{Mask generation results. (a) Main mask A (red overlay) defines the placement region on the base image where the object will be inserted. (b) Reference mask R isolates the target object (remote control) from the reference image, preserving its shape and boundaries.}
    \label{fig:mask_examples}
\end{figure}

\textbf{Example 1: Object Insertion}
For object insertion, the workflow begins with main mask creation on the base image. The user draws a bounding box or brush strokes on the base image (e.g., a wooden table) to define where the object should be placed. The frontend captures this input and sends it to the Segmentation Service. If BiRefNet is selected with brush input, FastSAM first generates an initial mask, then BiRefNet refines it on the calculated bounding box region. Next, reference object isolation occurs: the user uploads a reference image containing the target object (e.g., a remote control). Using the Reference Image Editor, they draw brush strokes over the object. The Segmentation Service processes these strokes; if using BiRefNet, it employs the two-stage FastSAM-then-BiRefNet approach to generate a precise mask. The Image Utils Service then extracts the object with a transparent/black background. Finally, the user provides a text prompt such as "a black remote control on a wooden table" and initiates generation. The backend positions the reference mask within the main mask region, prepares conditional images, and executes the diffusion process.

\textbf{Example 2: Object Removal}
For object removal, the workflow is simpler as no reference image is required. The user draws a bounding box or brush strokes on the base image to define the region containing the object to be removed. The Segmentation Service generates the main mask (Mask A) indicating the removal region. The user provides a text prompt describing the removal (e.g., "remove the object") and initiates generation. The backend prepares conditional images including the original image, the mask, and MAE representation, then executes the diffusion process to inpaint the masked region seamlessly.

\textbf{Example 3: White Balance Correction}
For white balance correction, no masks are required. The user simply uploads the input image and provides a text prompt describing the desired color correction (e.g., "adjust white balance to natural lighting"). The backend processes the entire image, generating Canny edge detection results as conditional input, and executes the diffusion process to apply global color temperature adjustments.

\textbf{Result Refinement}
For all tasks, the generated image is displayed, and users can adjust parameters (guidance scale, number of steps) and regenerate if needed. The system maintains debug sessions containing all intermediate images for analysis and troubleshooting.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/debug_info.png}
    \caption{Debug information panel showing all intermediate images from the generation pipeline. For object insertion tasks, the panel displays: (1) Original base image, (2) Main mask A, (3) Masked reference object (extracted object), (4) Original reference image, (5) Positioned mask R (reference mask after scaling and positioning), and (6) Final generated image. For object removal tasks, it shows: (1) Original base image, (2) Main mask A, (3) MAE representation, and (4) Final generated image. For white balance tasks, it displays: (1) Input image, (2) Canny edge detection, and (3) Final generated image. This comprehensive view enables users and developers to analyze each stage of the pipeline for debugging and optimization.}
    \label{fig:debug_info}
\end{figure}

\subsubsection{Runtime and Hardware Requirements}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/architecture_diagram.png}
    \caption{System architecture diagram showing the microservices deployment. The API Gateway routes requests to specialized services: Segmentation Service (T4 GPU) for mask generation, Image Utils Service (CPU) for object extraction, and Job Manager coordinating A100 GPU workers for image generation. Modal Volumes cache large models to reduce cold-start times. The architecture enables independent scaling of services based on workload. The diagram source code is available in \texttt{ARCHITECTURE\_DIAGRAM.mmd} and can be compiled using Mermaid CLI or rendered directly with the \texttt{mermaid} LaTeX package.}
    \label{fig:architecture}
\end{figure}

The system's performance characteristics are optimized through a microservices architecture with specialized hardware allocation. The API Gateway utilizes CPU-only containers to handle request routing and coordination, achieving cold-start times under 1 second. The Segmentation Service operates on T4 GPU instances with 16GB VRAM to process mask generation tasks, where FastSAM inference typically completes in 1-3 seconds, while BiRefNet requires 3-5 seconds including FastSAM preprocessing when brush inputs are used. The Image Generation Service executes the diffusion pipeline on A100 GPU instances with 40GB or 80GB VRAM. Generation times vary based on the number of inference steps (default: 10-20 steps), image resolution (typically 1024x1024 or user-specified dimensions), and model loading time, which is mitigated through Modal Volume caching, reducing cold-start from 20-60 seconds to 1-3 seconds. For a complete editing workflow encompassing mask generation, object extraction, and image generation, typical execution times range from 15-30 seconds on warm containers, with initial requests taking longer due to model loading.

The system employs several optimization strategies to enhance performance. Large models including Qwen, FastSAM, and BiRefNet are cached in Modal Volumes, enabling direct loading without network transfer overhead. BiRefNet and generation models utilize FP16 half-precision inference on GPU, reducing memory usage and improving inference speed. Long-running generation tasks are handled asynchronously with progress streaming, preventing frontend timeouts. Appropriate timeout configurations are implemented, with 5 minutes allocated for segmentation tasks and 30 minutes for generation tasks, accommodating variable processing times across different workloads.

\subsubsection{Potential Applications}

The multi-task image editing pipeline demonstrated in ArtMancer has broad applicability across multiple domains, with each task type serving distinct professional needs. For object insertion tasks, e-commerce and product visualization represent primary use cases, where retailers can generate product placement images by inserting products into various scene contexts, enabling rapid creation of marketing materials without physical photography setups. Interior design and architecture professionals can visualize furniture and decor items in real spaces, facilitating client presentations and design iterations. Real estate professionals can virtually stage properties by inserting furniture and decor, reducing costs associated with physical staging. Fashion designers can visualize garments in different contexts or combine elements from multiple designs. Artists and content creators can combine elements from different sources to create composite images, expanding creative possibilities in digital art.

For object removal tasks, the system enables efficient background cleanup and unwanted element elimination across various industries. Photography and media production professionals can remove unwanted objects, people, or artifacts from images without time-consuming manual editing, streamlining post-production workflows. Real estate photographers can remove temporary items, personal belongings, or visual distractions from property photos, creating cleaner marketing materials. E-commerce platforms can eliminate background clutter from product images, ensuring consistent and professional product presentations. Urban planning and architectural visualization professionals can remove temporary construction elements or unwanted structures from site photographs, creating cleaner documentation. Social media content creators can remove unwanted elements from photos, enhancing visual appeal without complex editing software.

For white balance correction tasks, the system addresses color accuracy and lighting consistency challenges across professional photography and media production. Professional photographers can correct color temperature issues caused by mixed lighting conditions, ensuring accurate color representation in final images. Product photographers can standardize color appearance across product catalogs, maintaining brand consistency in e-commerce and marketing materials. Real estate photographers can correct color casts from indoor lighting, presenting properties in natural and appealing lighting conditions. Fashion and lifestyle photographers can ensure accurate skin tones and fabric colors, meeting industry standards for editorial and commercial work. Video production teams can correct color inconsistencies between shots, maintaining visual continuity in film and television production. Medical and scientific imaging professionals can improve color accuracy in documentation, ensuring precise representation of specimens and materials.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/application_ecommerce.png}
        \caption{Object insertion: E-commerce product placement}
        \label{fig:app_ecommerce}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/application_whitebalance.png}
        \caption{White balance correction: Color temperature adjustment}
        \label{fig:app_whitebalance}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/application_content.png}
        \caption{Object removal: Content cleanup}
        \label{fig:app_content}
    \end{subfigure}
    \caption{Potential application domains across three task types. (a) Object insertion: Products can be placed in various scene contexts for marketing materials. (b) White balance correction: Color temperature and lighting conditions adjusted for accurate color representation. (c) Object removal: Unwanted elements removed from images for cleaner content.}
    \label{fig:applications}
\end{figure}

The system's multi-modal input capabilities (brush, bounding box, automatic detection) make it accessible to users with varying levels of technical expertise, while the backend's sophisticated AI pipeline ensures high-quality results suitable for professional applications.

